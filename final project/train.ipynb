{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy.matlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from utils import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# declare parameters, same as the inference declaration\n",
    "alphabet = ' abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "rnn_size = 400\n",
    "tsteps = 150\n",
    "batch_size = 32\n",
    "placeholder_shape = [None, tsteps, 3]\n",
    "kmixtures = 1\n",
    "nmixtures = 8\n",
    "v_len = len(alphabet) + 1 #plus one for <UNK> token\n",
    "tsteps_per_ascii =25\n",
    "text_length = tsteps // tsteps_per_ascii\n",
    "save_path = './saved/model.ckpt'\n",
    "data_dir = './data'\n",
    "eos_prob = 0.4 # threshold probability for ending a stroke\n",
    "train = True\n",
    "data_scale = 50\n",
    "grad_clip = 10.0\n",
    "dropout = 0.85\n",
    "optimizer = \"rmsprop\"\n",
    "num_layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data found, proceed to loading\n",
      "\tloaded dataset:\n",
      "\t\t11315 train individual data points\n",
      "\t\t595 valid individual data points\n",
      "\t\t353 batches\n"
     ]
    }
   ],
   "source": [
    "# logger = Logger(log_dir, train)\n",
    "dataloader = DataLoader(data_dir, alphabet, batch_size, tsteps, data_scale, tsteps_per_ascii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initial weight vector proposed in Alex_Graves Paper\n",
    "LSTM_initializer = tf.truncated_normal_initializer(mean=0., stddev=.075, seed=None, dtype=tf.float32)\n",
    "\n",
    "window_b_initializer = tf.truncated_normal_initializer(mean=-3.0, stddev=.25, seed=None, dtype=tf.float32)\n",
    "\n",
    "cell = [None] * num_layers\n",
    "\n",
    "for i in range(num_layers) :\n",
    "    cell[i] = tf.contrib.rnn.LSTMCell(rnn_size, state_is_tuple=True, initializer=LSTM_initializer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_data = tf.placeholder(dtype=tf.float32, shape=placeholder_shape)\n",
    "output_data = tf.placeholder(dtype=tf.float32, shape=placeholder_shape)\n",
    "istate_cell = [None] *num_layers\n",
    "outs_cell = [None] * num_layers\n",
    "fstate_cell = [None] * num_layers\n",
    "\n",
    "for i in range(num_layers) :\n",
    "    istate_cell[i] = cell[i].zero_state(batch_size=batch_size, dtype=tf.float32)\n",
    "    \n",
    "inputs = [tf.squeeze(i, [1]) for i in tf.split(input_data, tsteps, 1)]\n",
    "\n",
    "outs_cell[0], fstate_cell[0] = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, istate_cell[0], cell[0], loop_function=None, scope='cell0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#attention mechanism\n",
    "def get_phi(length, a, b, k):\n",
    "    u = np.linspace(0, length-1 , length) \n",
    "    e = tf.multiply(b, - tf.square(tf.subtract(k,u)))\n",
    "    phi = tf.multiply(a, tf.exp(e))\n",
    "    return tf.reduce_sum(phi, 1, keep_dims=True)\n",
    "\n",
    "# get the soft window \n",
    "def get_window(coef):\n",
    "    [a, b, k, c] = coef\n",
    "    length = c.get_shape()[1].value #number of items in sequence\n",
    "    phi = get_phi(length, a, b, k)\n",
    "    window = tf.squeeze(tf.matmul(phi,c), [1])\n",
    "    return window, phi\n",
    "\n",
    "# soft window parameters \n",
    "def get_coef(i, out_cell, kmixtures, prev_k, char_seq, reuse=True):\n",
    "    hidden = out_cell.get_shape()[1]\n",
    "    n_out = 3*kmixtures\n",
    "    with tf.variable_scope('window',reuse=reuse):\n",
    "        window_w = tf.get_variable(\"window_w\", [hidden, n_out], initializer=LSTM_initializer)\n",
    "        window_b = tf.get_variable(\"window_b\", [n_out], initializer=window_b_initializer)\n",
    "    co = tf.nn.xw_plus_b(out_cell, window_w, window_b) \n",
    "    abk = tf.exp(tf.reshape(co, [-1, 3*kmixtures,1]))\n",
    "    a, b, k = tf.split(abk, 3, 1) \n",
    "    k = k + prev_k\n",
    "    return a, b, k, char_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initial parameters\n",
    "init_kappa = tf.placeholder(dtype=tf.float32, shape=[None, kmixtures, 1]) \n",
    "char_seq = tf.placeholder(dtype=tf.float32, shape=[None, text_length, v_len])\n",
    "wavg_prev_kappa = init_kappa\n",
    "prev_window = char_seq[:,0,:]\n",
    "\n",
    "#add soft window to the top of the first LSTM layer \n",
    "reuse = False\n",
    "for i in range(len(outs_cell[0])):\n",
    "    coef = get_coef(i, outs_cell[0][i], kmixtures, wavg_prev_kappa, char_seq,  reuse=reuse)\n",
    "    (_, _, next_kappa, _) = coef\n",
    "    window, phi = get_window(coef)\n",
    "    #combine first layer output, soft-window, and original input text\n",
    "    outs_cell[0][i] = tf.concat((outs_cell[0][i], window, inputs[i]), 1)\n",
    "    wavg_prev_kappa = tf.reduce_mean(next_kappa, reduction_indices=1, keep_dims=True) # mean along kmixtures dimension\n",
    "    reuse = True\n",
    "\n",
    "(alpha, beta, next_kappa, _) = coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----- finish building second recurrent cell\n",
    "for i in range(1, num_layers):\n",
    "    scope = 'cell' + str(i)\n",
    "    outs_cell[i], fstate_cell[i]= tf.contrib.legacy_seq2seq.rnn_decoder(outs_cell[i-1], istate_cell[i], cell[i], \\\n",
    "                                                    loop_function=None, scope=scope) #use scope from training\n",
    "\n",
    "r_out = tf.reshape(tf.concat(outs_cell[num_layers - 1], 1), [-1, rnn_size]) #concat outputs for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#put a dense cap on top of the rnn cells (to interface with the mixture density network)\n",
    "n_out = 1 + nmixtures * 6 # params = end_of_stroke + 6 parameters per Gaussian\n",
    "with tf.variable_scope('mdn_dense'):\n",
    "    output_w = tf.get_variable(\"output_w\", [rnn_size, n_out], initializer=LSTM_initializer)\n",
    "    output_b = tf.get_variable(\"output_b\", [n_out], initializer=LSTM_initializer)\n",
    "\n",
    "output = tf.nn.xw_plus_b(r_out, output_w, output_b) #data flows through dense nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MDN above the last LSTM layer\n",
    "def gaussian2d(x1, x2, m1, m2, s1, s2, r):\n",
    "    # define gaussian mdn (eq 24, 25 from http://arxiv.org/abs/1308.0850)\n",
    "    (sub1, sub2) = (tf.subtract(x1, m1), tf.subtract(x2, m2))\n",
    "    sum_1 = tf.square(tf.div(sub1, s1)) + tf.square(tf.div(sub2, s2))    \n",
    "    Z = sum_1 - 2*tf.div(tf.multiply(rho, tf.multiply(sub1, sub2)), tf.multiply(s1, s2))\n",
    "    reg = 2*np.pi*tf.multiply(tf.multiply(s1, s2), tf.sqrt(1 - tf.square(r)))\n",
    "    gaussian = tf.div(tf.exp(tf.div(-Z,2* (1 - tf.square(r)))), reg)\n",
    "    return gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coefficient for MDN layer, detail function can be found in related papers\n",
    "def get_mdn_coef(Z):\n",
    "    global pi_hat, m1_hat, m2_hat, s1_hat, s2_hat, r_hat\n",
    "    pi_hat, m1_hat, m2_hat, s1_hat, s2_hat, r_hat = tf.split(Z[:, 1:], 6, 1)\n",
    "    eos = tf.sigmoid(-1*Z[:, 0:1])\n",
    "    pi = tf.nn.softmax(pi_hat) # softmax\n",
    "    m1 = m1_hat; m2 = m2_hat # leave mu1, mu2 as they are\n",
    "    s1 = tf.exp(s1_hat); s2 = tf.exp(s2_hat) # exp for sigmas\n",
    "    r = tf.tanh(r_hat) # tanh for rho (squish between -1 and 1)\n",
    "\n",
    "    return [[eos, pi, m1_hat, m2_hat, tf.exp(s1_hat), tf.exp(s2_hat), tf.tanh(r_hat)], [pi_hat, m1_hat, m2_hat, s1_hat, s2_hat, r_hat]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss function from the paper\n",
    "def get_loss(pi, x1_data, x2_data, eos_data, mu1, mu2, sigma1, sigma2, rho, eos):\n",
    "    gaussian = gaussian2d(x1_data, x2_data, mu1, mu2, sigma1, sigma2, rho)\n",
    "    term1 = tf.reduce_sum(tf.multiply(gaussian, pi), 1, keep_dims=True)\n",
    "    term1 = -tf.log(tf.maximum(term1, 1e-20))\n",
    "    term2 = -tf.log(tf.multiply(eos, eos_data) + tf.multiply(1-eos, 1-eos_data))\n",
    "    return tf.reduce_sum(term1 + term2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flat_target_data = tf.reshape(output_data,[-1, 3])\n",
    "[x1_data, x2_data, eos_data] = tf.split(flat_target_data, 3, 1)\n",
    "retval = get_mdn_coef(output)\n",
    "[eos, pi, mu1, mu2, sigma1, sigma2, rho] = retval[0]\n",
    "pi_hat, mu1_hat, mu2_hat, sigma1_hat, sigma2_hat, rho_hat = retval[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = get_loss(pi, x1_data, x2_data, eos_data, mu1, mu2, sigma1, sigma2, rho, eos)\n",
    "cost = loss / (batch_size * tsteps)\n",
    "\n",
    "# initial variables for training\n",
    "m_learning_rate = tf.Variable(0.0, trainable=False)\n",
    "m_decay = tf.Variable(0.0, trainable=False)\n",
    "m_momentum = tf.Variable(0.0, trainable=False)\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "\n",
    "if optimizer == 'adam':\n",
    "    m_optimizer = tf.train.AdamOptimizer(learning_rate=m_learning_rate)\n",
    "elif optimizer == 'rmsprop':\n",
    "    m_optimizer = tf.train.RMSPropOptimizer(learning_rate=m_learning_rate, decay=m_decay, momentum=m_momentum)\n",
    "else:\n",
    "    raise ValueError(\"Optimizer type not recognized\")\n",
    "train_op = m_optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "#load data from files\n",
    "input, output, _ , seq = dataloader.validation_data()\n",
    "valid_inputs = {input_data: input, output_data: output, char_seq: seq}\n",
    "\n",
    "#initialize training\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver(tf.global_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "0/50000\n"
     ]
    }
   ],
   "source": [
    "print(\"start training...\")\n",
    "#misc parameters for training\n",
    "momentum = 0.9\n",
    "decay = 0.95\n",
    "remember_rate = 0.99\n",
    "nepochs = 100\n",
    "learning_rate = 1e-4\n",
    "lr_decay = 1.0\n",
    "nbatches = 500\n",
    "save = 500\n",
    "total_step = nepochs * nbatches\n",
    "\n",
    "#initialize the network\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.assign(m_decay, decay ))\n",
    "sess.run(tf.assign(m_momentum, momentum ))\n",
    "\n",
    "for e in range(0, nepochs):\n",
    "    sess.run(tf.assign(m_learning_rate, learning_rate * (lr_decay ** e)))\n",
    "    \n",
    "    c = [None] * num_layers\n",
    "    h = [None] * num_layers\n",
    "    for counter in range(num_layers):\n",
    "        c[counter] = istate_cell[counter].c.eval()\n",
    "        h[counter] = istate_cell[counter].h.eval()\n",
    "    \n",
    "    kappa = np.zeros((batch_size, kmixtures, 1))\n",
    "\n",
    "    for b in range(nbatches):\n",
    "        \n",
    "        # current step\n",
    "        i = e * nbatches + b\n",
    "            \n",
    "        #save model for every given point\n",
    "        if i % save == 0 and (i != 0):\n",
    "            saver.save(sess, save_path, global_step = i)\n",
    "            print(\"model saved at\" + str(i))\n",
    "        \n",
    "        #load next batch for training\n",
    "        x, y, s, ch = dataloader.next_batch()\n",
    "\n",
    "        #feed the training set into network\n",
    "        feed = {input_data: x, output_data: y, char_seq: ch, init_kappa: kappa}\n",
    "        for j in range(num_layers):\n",
    "            feed[istate_cell[j].c] = c[j]\n",
    "            feed[istate_cell[j].h] = h[j]\n",
    "        \n",
    "        #run the network\n",
    "        sess.run([cost, train_op], feed)\n",
    "        feed.update(valid_inputs)\n",
    "        feed[init_kappa] = np.zeros((batch_size, kmixtures, 1))\n",
    "        sess.run([cost], feed)\n",
    "\n",
    "        #print out the process\n",
    "        if i % 10 == 0: \n",
    "            print(str(i) + '/' + str(total_step))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
